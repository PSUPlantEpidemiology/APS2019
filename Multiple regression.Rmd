---
title: "Multiple regression"
author: "Paul Esker and Felipe Dalla Lana"
output:
  word_document:
    toc: yes
  html_document:
    df_print: paged
    fontsize: 11pt
    geometry: margin=1in
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, message=FALSE)
options(width=999)
```

## Background

Given the background and tools presented in linear regression, we will not extend the modeling approach to include additional variables, as well as relationships that are more complicated. This exercise provides the jumping off point for more automated modeling approaches, which will we see in the subsequent example(s). 

Our assumption in this exercise is that multiple factors have explanatory value to explain the response variable of interest. 

What does a model of this type look like? Some examples include:

1. Additive.

$Y = \beta_0 + \beta_1{X_1} + \beta_2{X_2} + \epsilon$

2. With interaction between the two terms.

$Y = \beta_0 + \beta_1{X_1} + \beta_2{X_2} + \beta_3{X_1}{X_2} + \epsilon$

Note: It is important to note that in modeling, when we add new explanatory variables that have merit (i.e., the sign makes sense in terms of the biological relation), the model will improve. This is not necessarily the same as being "biologically relevant". We should always consider the variable in the context of the question of interest. 

```{r packages}

library(tidyverse)
library(Hmisc)
library(corrplot)
library(readr)
library(HH)
library(car)
library(scatterplot3d)
library(olsrr)

```

## Data and exploratory analysis

Our database comes from counts of the number of aphids in different lots, as well as measures of average temperature and relative humidity. We assume that there is a relationship between those two latter factors with the observed number of aphids, which means from a predictive value, we hope that by just measuring T and RH, we can estimate the number of expected aphids.  

Where do we start? The main question is to determine if there is (are) a relationship between T and RH with the counts. We are also interested in trying to determine if there may be a complext relationship (i.e., that the predictive values have some degree of interpretable interaction). 

```{r data}

lot <- c(1:34)
aphids <- c(61, 77, 87, 93, 98, 100, 104, 118, 102, 74, 63, 43, 27, 19, 14, 23, 30, 25, 67, 40, 6, 21, 18, 23, 42, 56, 60, 59, 82, 89, 77, 102, 108, 97)
temperature <- c(21, 24.8, 28.3, 26, 27.5, 27.1, 26.8, 29, 28.3, 34, 30.5, 28.3, 30.8, 31, 33.6, 31.8, 31.3, 33.5, 33, 34.5, 34.3, 34.3, 33, 26.5, 32, 27.3, 27.8, 25.8, 25, 18.5, 26, 19, 18, 16.3)
humidity <- c(57,48, 41.5, 56, 58, 31, 36.5, 41, 40, 25, 34, 13, 37, 19, 20, 17, 21, 18.5, 24.5, 16, 6, 26, 21, 26, 28, 24.5, 39, 29, 41, 53.5, 51, 48, 70, 79.5)

aphids_data <- data.frame(lot, aphids, temperature, humidity)

# Quick exploratory analysis
summary(aphids_data)
cor(aphids_data[,2:4])
plot(aphids_data[,2:4]) # Graphical matrix
pairs(aphids_data[,2:4]) # Gives us the same thing

```

## Linear regression

```{r linear}

# Factor = temperature (X)

model1<-with(aphids_data, lm(aphids~temperature))
anova(model1)
summary(model1)
plot(model1)

# Assumptions = values, model1

rstudent(model1)
dfbetas(model1)
dffits(model1)
covratio(model1)
cooks.distance(model1)

# Factor = humedad (X)

model2<-with(aphids_data, lm(aphids~humidity))
anova(model2)
summary(model2)
plot(model2)

# Assumptions = values, model2

rstudent(model2)
dfbetas(model2)
dffits(model2)
covratio(model2)
cooks.distance(model2)

```

## Additive multiple regression

Model form:

$$aphids = intercept + temperature + humidity + error$$ 

```{r TRH}

model3<-with(aphids_data, lm(aphids~temperature+humidity))
anova(model3)
summary(model3)
plot(model3)
vif(lm(aphids~temperature+humidity, data=aphids_data))

# Assumptions = values, model3
rstudent(model3)
dfbetas(model3)
dffits(model3)
covratio(model3)
cooks.distance(model3)

```

## Visualizing more complicated relationships

So far, we cannot say with certainty that the additive model is the best fitting model. Before we commit to another analysis, it is important to take a step back and think about the visualization of the data to be better informed about what has occurred. Another reason for doing this is to be able to better interpret the observed results about the model assumptions (i.e., influential observations, some unhidden spatial structure in the data collection process).

```{r visualization}

# Start with temperature, let's add to the graph infomration about the lots
temp <- ggplot(aphids_data, aes(x=temperature, y=aphids, label=lot))
temp + geom_point() + geom_text(hjust=0.5, nudge_y=3) #Have a look a few of the observations like 30, 32, 33, 34 and also 8 (maybe 9)


# Now let's consider RH and do the same thing
temp2 <- ggplot(aphids_data, aes(x=humidity, y=aphids, label=lot))
temp2 + geom_point() + geom_text(hjust=0.5, nudge_y=3) #Maybe a bit different grouping" 6-9, 33 and 34

# In 3-dimensiones? This example comes from the package *scatterplot3d*

with(aphids_data, scatterplot3d(temperature, humidity, aphids, angle=75)) 

```

## Multiple regression with interactions

Given that individually, we see different relationships between the number of aphids with temperature or relative humidity, we might want to consider if there is an interaction between those two factors that helps to explain the overall relationship.  

```{r interaction}

model4 <- with(aphids_data, lm(aphids ~ temperature + humidity + temperature:humidity))

anova(model4)
summary(model4) 
plot(model4)

# Assumptions = values, model4
rstudent(model4)
dfbetas(model4)
dffits(model4)
covratio(model4)
cooks.distance(model4)

# Graphically from olsrr package
ols_plot_resid_stud(model4)
ols_plot_dfbetas(model4)
ols_plot_dffits(model4)
ols_plot_cooksd_chart(model4)

# Compare the different models
anova(model1, model3) # model 3 better
anova(model2, model3) # model 2 mejor (only RH)
anova(model2, model4) # the interaction improved the model?
anova(model3, model4) # the interaction improved the model

# Remember that once we have a model selected, we should examine the assumptions in greater detail
```

## Predictions

To close our discussion, let's again look at the function *predict* using model 4.

```{r predictions}

# Let's start by considering the average values for temperature and relative humidity
mean(temperature)
mean(humidity)

observation <- data.frame(temperature=mean(temperature), humidity=mean(humidity))

predict(object=model4, newdata=observation, interval="confidence")
predict(object=model4, newdata=observation, interval="predict")

# Looking at all observations in the database
intervals<-predict(model4, interval="confidence")
intervals

predictions<-predict(model4, interval="predict")
predictions
```

## Summary

The objective in this exercise was to introduce the concept of using multiple regression to build a model. This provides a base also as you move forward in your modeling work to think about things like *hidden interactions*, which is often very common in complex datasets and can often drive aspects of things like machine learning.